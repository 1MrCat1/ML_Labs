{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение. ЛР №2\n",
    "### Журавлёв Константин, М8О-408Б-17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт всех нужных библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Собственные классы для различных классификаторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg:\n",
    "    def __init__(self, lr=0.05, iters=5000):\n",
    "        self.learningRate = lr\n",
    "        self.iterations = iters\n",
    "    \n",
    "    def intercept(self, X):\n",
    "        return np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def lossFunction(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = self.intercept(X)\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        \n",
    "        for i in range(self.iterations):\n",
    "            z = np.dot(X, self.w)\n",
    "            h = self.sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            self.w -= self.learningRate * gradient\n",
    "            \n",
    "        z = np.dot(X, self.w)\n",
    "        h = self.sigmoid(z)\n",
    "        loss = self.lossFunction(h, y)\n",
    "            \n",
    "    \n",
    "    def predict_probability(self, X):\n",
    "        X = self.intercept(X)\n",
    "        return self.sigmoid(np.dot(X, self.w))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.predict_probability(X).round()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"LogReg.\\n\\t\" + \"LearningRate = \" + str(self.learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def fit(self, X_train, Y_train):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "\n",
    "    def euclideanNorm(self, row, column):\n",
    "        temp = (row-column)\n",
    "        return np.sqrt((temp**2).sum())\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for row in X_test:\n",
    "            index = self.closest(row)\n",
    "            predictions.append(index)\n",
    "        return predictions\n",
    "\n",
    "    def closest(self, row):\n",
    "        bestDist = self.euclideanNorm(row, self.X_train[0])\n",
    "        bestIndex = 0\n",
    "        for i in range(1, len(self.X_train)):\n",
    "            dist = self.euclideanNorm(row, self.X_train[i])\n",
    "            if dist < bestDist:\n",
    "                bestDist = dist\n",
    "                bestIndex = i\n",
    "        return self.Y_train[bestIndex]\n",
    "    def __str__(self):\n",
    "        return \"KNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, pclass):\n",
    "        self.predicted = pclass\n",
    "        self.featureIdx = 0\n",
    "        self.border = 0\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "class DecisionTree():\n",
    "    def __init__(self, maxDepth = 1, rf = False):\n",
    "        self.maxDepth = maxDepth\n",
    "        self.rf = rf\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.nClasses = len(set(y))\n",
    "        self.features = X.shape[1]\n",
    "        self.tree = self.UpdateTree(X, y)\n",
    "    \n",
    "    def Split(self, X, y):\n",
    "        m = y.size    \n",
    "        if m <= 1:\n",
    "            return None, None\n",
    "        parent = [np.sum(y == c) for c in range(self.nClasses)]\n",
    "        bGini = 1.0 - sum((n / m) ** 2 for n in parent)\n",
    "        bIdx, bThr = None, None\n",
    "        for idx in range(self.features):\n",
    "            borders, types = zip(*sorted(zip(X[:, idx], y)))\n",
    "            left = [0] * self.nClasses\n",
    "            right = parent.copy()\n",
    "            for i in range(1, m):\n",
    "                c = types[i - 1]\n",
    "                right[c] -= 1\n",
    "                left[c] += 1\n",
    "                giniLeft = 1.0 - sum((left[x] / i) ** 2 for x in range(self.nClasses))\n",
    "                giniRight = 1.0 - sum((right[x] / (m - i)) ** 2 for x in range(self.nClasses))\n",
    "                gini = (i * giniLeft + (m - i) * giniRight) / m\n",
    "                if borders[i] == borders[i - 1]:\n",
    "                    continue\n",
    "                if gini < bGini:\n",
    "                    bGini = gini\n",
    "                    bIdx = idx\n",
    "                    bThr = (borders[i] + borders[i - 1]) / 2\n",
    "        return bIdx, bThr\n",
    "\n",
    "    def UpdateTree(self, X, y, depth = 0):\n",
    "        samplesPerClass = [np.sum(y == i) for i in range(self.nClasses)]\n",
    "        predType = np.argmax(samplesPerClass)\n",
    "        node = Node(pclass = predType)\n",
    "        if depth < self.maxDepth:      \n",
    "            idx, thr = self.Split(X, y)\n",
    "            if idx is not None:\n",
    "                leftIdx = X[:, idx] < thr\n",
    "                lx, ly = X[leftIdx], y[leftIdx]\n",
    "                rx, ry = X[~leftIdx], y[~leftIdx]\n",
    "                node.featureIdx = idx\n",
    "                node.border = thr\n",
    "                node.left = self.UpdateTree(lx, ly, depth + 1)\n",
    "                node.right = self.UpdateTree(rx, ry, depth + 1)\n",
    "        return node\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return [self.predictForSingleInput(Input) for Input in X]\n",
    "    \n",
    "    def predictForSingleInput(self, X):\n",
    "        node = self.tree\n",
    "        while node.left:\n",
    "            if X[node.featureIdx] < node.border:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.predicted    \n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"DecisionTree.\\n\\tMaxDepth = \" + str(self.maxDepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, max_depth=5, n_estimators=100, max_features=None):\n",
    "        self.maxDepth = max_depth\n",
    "        self.maxFeatures = max_features\n",
    "        self.nEstimators = n_estimators\n",
    "        self.forest = [None] * n_estimators\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.nEstimators):\n",
    "            self.forest[i] = DecisionTree(\n",
    "                self.maxDepth, rf=True) \n",
    "            self.forest[i].fit(X, y)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        mostCommon = np.zeros(X.shape[0])\n",
    "        preds = np.zeros((self.nEstimators, X.shape[0]))\n",
    "        for i in range(self.nEstimators):\n",
    "            preds[i] = self.forest[i].predict(X)\n",
    "        for i in range(len(mostCommon)):\n",
    "            mostCommon[i] = Counter(preds[:, i]).most_common(1)[0][0]\n",
    "        return mostCommon.astype(int)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"RandomForest.\\n\\tMaxDepth = \" + str(self.maxDepth) + \"\\n\\tEstimators = \" + str(self.nEstimators) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Создадим новую функцию для аггрегации метрик обучения двух методов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def CalcMetrics(Method1,Method2,X_train,Y_train,X_test,Y_test):\n",
    "    s11 = time.clock()\n",
    "    Method1.fit(X_train.values,Y_train.values)\n",
    "    e11 = time.clock()\n",
    "    s21 = time.clock()\n",
    "    Method2.fit(X_train.values,Y_train.values)\n",
    "    e21 = time.clock()\n",
    "    \n",
    "    s12 = time.clock()\n",
    "    Y_pred1 = Method1.predict(X_test.values)\n",
    "    Y_tr1 = Method1.predict(X_train.values)\n",
    "    e12 = time.clock()\n",
    "    s22 = time.clock()\n",
    "    Y_pred2 = Method2.predict(X_test.values)\n",
    "    Y_tr2 = Method2.predict(X_train.values)\n",
    "    e22 = time.clock()\n",
    "    \n",
    "    \n",
    "    precision1= precision_score(Y_pred1, Y_test, average = 'macro')\n",
    "    recall1= recall_score(Y_pred1, Y_test, average = 'macro')\n",
    "    trainAc1= accuracy_score(Y_tr1, Y_train)\n",
    "    testAc1= accuracy_score(Y_pred1, Y_test)\n",
    "    \n",
    "    precision2= precision_score(Y_pred2, Y_test, average = 'macro')\n",
    "    recall2= recall_score(Y_pred2, Y_test, average = 'macro')\n",
    "    trainAc2= accuracy_score(Y_tr2, Y_train)\n",
    "    testAc2= accuracy_score(Y_pred2, Y_test)\n",
    "    \n",
    "    print(Method1)\n",
    "    print('Fit time = ', e11-s11)\n",
    "    print('Prediction time = ', e12-s12)\n",
    "    print(\"precision:\", precision1)\n",
    "    print(\"recall:\", recall1)\n",
    "    print(\"train_accuracy:\", trainAc1)\n",
    "    print(\"test_accuracy:\", testAc1)\n",
    "    \n",
    "    print(Method2)\n",
    "    print('Fit time = ', e21-s21)\n",
    "    print('Prediction time = ', e22-s22)\n",
    "    print(\"precision:\", precision2)\n",
    "    print(\"recall:\", recall2)\n",
    "    print(\"train_accuracy:\", trainAc2)\n",
    "    print(\"test_accuracy:\", testAc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шахматы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начинаем тестирование на первом датасете - шахматы.\n",
    "Из-за того, что классифицируем мы только на 2 класса, необходимо убрать из данных все вхождения партий с ничейным исходом, перенумеровать второй исход, а также последний столбец с объектным содержимим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Chess_ready.csv')\n",
    "df.head()\n",
    "df.drop('increment_code',axis=1,inplace = True)\n",
    "for i in range(df.index.size):\n",
    "    if df.at[i,'winner']==1:\n",
    "        df.drop([i],axis=0,inplace=True)\n",
    "        continue\n",
    "    if df.at[i,'winner']==2:\n",
    "        df.at[i,'winner'] = 1\n",
    "#print(df)\n",
    "#df.reindex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём выборки для обучения и тестирования. Первая - полная, вторая - ограничена 1000 строками, т.к. некоторые методы работают слишком долго.\n",
    "Примечание. Из-за того, что пришлось удалять строки из датасета, возможно получить ошибку out_of_bounds при обучении дерева решения и случайного леса на малой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(df, test_size=0.2)\n",
    "X_train = train_set.drop('winner', axis=1).astype('float32')\n",
    "X_test = test_set.drop('winner', axis=1).astype('float32')\n",
    "Y_train = train_set.winner\n",
    "Y_test = test_set.winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(df[df.index <= 1000], test_size=0.3)\n",
    "X_train = train_set.drop('winner', axis=1)\n",
    "X_test = test_set.drop('winner', axis=1)\n",
    "Y_train = train_set.winner\n",
    "Y_test = test_set.winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg.\n",
      "\tLearningRate = 0.2\n",
      "Fit time =  0.4972533460000008\n",
      "Prediction time =  0.0005038219999988769\n",
      "precision: 0.5\n",
      "recall: 0.2719298245614035\n",
      "train_accuracy: 0.5045317220543807\n",
      "test_accuracy: 0.543859649122807\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Fit time =  0.001524572999997531\n",
      "Prediction time =  0.0004163370000043187\n",
      "precision: 0.5\n",
      "recall: 0.2719298245614035\n",
      "train_accuracy: 0.5045317220543807\n",
      "test_accuracy: 0.543859649122807\n",
      "Wall time: 512 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CalcMetrics(LogReg(lr=0.2),LogisticRegression(),X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "Fit time =  0.00030234999999834145\n",
      "Prediction time =  9.019446428999998\n",
      "precision: 0.5254342431761787\n",
      "recall: 0.52525376958707\n",
      "train_accuracy: 1.0\n",
      "test_accuracy: 0.5263157894736842\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
      "                     weights='uniform')\n",
      "Fit time =  0.0016835850000020969\n",
      "Prediction time =  0.04968597999999247\n",
      "precision: 0.5254342431761787\n",
      "recall: 0.52525376958707\n",
      "train_accuracy: 1.0\n",
      "test_accuracy: 0.5263157894736842\n",
      "Wall time: 9.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CalcMetrics(KNN(),KNeighborsClassifier(n_neighbors=1),X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree.\n",
      "\tMaxDepth = 5\n",
      "Fit time =  0.6388217869999977\n",
      "Prediction time =  0.003912596000006374\n",
      "precision: 0.5764267990074441\n",
      "recall: 0.5776053215077606\n",
      "train_accuracy: 0.6918429003021148\n",
      "test_accuracy: 0.5824561403508772\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best')\n",
      "Fit time =  0.0058164599999912525\n",
      "Prediction time =  0.0010774599999905377\n",
      "precision: 0.5887096774193548\n",
      "recall: 0.5914322250639386\n",
      "train_accuracy: 0.6918429003021148\n",
      "test_accuracy: 0.5964912280701754\n",
      "Wall time: 660 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CalcMetrics(DecisionTree(maxDepth = 5),DecisionTreeClassifier(max_depth=5),X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest.\n",
      "\tMaxDepth = 5\n",
      "\tEstimators = 100\n",
      "Fit time =  61.45304100999999\n",
      "Prediction time =  0.3098465480000243\n",
      "precision: 0.5764267990074441\n",
      "recall: 0.5776053215077606\n",
      "train_accuracy: 0.6918429003021148\n",
      "test_accuracy: 0.5824561403508772\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "Fit time =  0.20463504400001398\n",
      "Prediction time =  0.027835570999997117\n",
      "precision: 0.6208436724565757\n",
      "recall: 0.6256709331131296\n",
      "train_accuracy: 0.797583081570997\n",
      "test_accuracy: 0.6105263157894737\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CalcMetrics(RandomForest(),RandomForestClassifier(n_estimators=100, max_depth=5),X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выводы по первому датасету"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно видеть, что написанные вручную методы по точности предсказаний не уступают методам из sklearn. Но вот временная эффективность на самой затратной для данного метода итерации (обучения или предсказания) отличается примерно в 100 (!!!) раз. Общий же для обоих типов сетей результат точности не выше 60% можно объяснить независимостью данных, и искомого признака - победы белых или чёрных. Это было отлично видно на тепловой карте в 1ой лабораторной, поэтому тут удивляться нечему."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('LoL_ready.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(df2, test_size=0.3)\n",
    "X_train = train_set.drop('blueWins', axis=1)\n",
    "X_test = test_set.drop('blueWins', axis=1)\n",
    "Y_train = train_set.blueWins\n",
    "Y_test = test_set.blueWins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(df2[df2.index <= 1000], test_size=0.3)\n",
    "X_train = train_set.drop('blueWins', axis=1)\n",
    "X_test = test_set.drop('blueWins', axis=1)\n",
    "Y_train = train_set.blueWins\n",
    "Y_test = test_set.blueWins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg.\n",
      "\tLearningRate = 0.2\n",
      "Fit time =  0.5738362170000073\n",
      "Prediction time =  0.0006577040000195211\n",
      "precision: 0.5\n",
      "recall: 0.26245847176079734\n",
      "train_accuracy: 0.4785714285714286\n",
      "test_accuracy: 0.5249169435215947\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Fit time =  0.0023509770000202934\n",
      "Prediction time =  0.0005411519999825032\n",
      "precision: 0.5\n",
      "recall: 0.23754152823920266\n",
      "train_accuracy: 0.5214285714285715\n",
      "test_accuracy: 0.4750830564784053\n",
      "Wall time: 590 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CalcMetrics(LogReg(lr=0.2),LogisticRegression(),X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "Fit time =  0.000292661000003136\n",
      "Prediction time =  9.995464465999987\n",
      "precision: 0.5267327609099761\n",
      "recall: 0.5267730496453901\n",
      "train_accuracy: 1.0\n",
      "test_accuracy: 0.5282392026578073\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
      "                     weights='uniform')\n",
      "Fit time =  0.0019691210000019055\n",
      "Prediction time =  0.05165168299998868\n",
      "precision: 0.5267327609099761\n",
      "recall: 0.5267730496453901\n",
      "train_accuracy: 1.0\n",
      "test_accuracy: 0.5282392026578073\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CalcMetrics(KNN(),KNeighborsClassifier(n_neighbors=1),X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree.\n",
      "\tMaxDepth = 5\n",
      "Fit time =  1.6168206300000065\n",
      "Prediction time =  0.003459497999983796\n",
      "precision: 0.6506152075772329\n",
      "recall: 0.6516488413547237\n",
      "train_accuracy: 0.83\n",
      "test_accuracy: 0.6478405315614618\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best')\n",
      "Fit time =  0.009790038999994977\n",
      "Prediction time =  0.00099624399999243\n",
      "precision: 0.6474506506152076\n",
      "recall: 0.648661311914324\n",
      "train_accuracy: 0.83\n",
      "test_accuracy: 0.6445182724252492\n",
      "Wall time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CalcMetrics(DecisionTree(maxDepth = 5),DecisionTreeClassifier(max_depth=5),X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest.\n",
      "\tMaxDepth = 5\n",
      "\tEstimators = 100\n",
      "Fit time =  163.89184136800003\n",
      "Prediction time =  0.380558528999984\n",
      "precision: 0.6506152075772329\n",
      "recall: 0.6516488413547237\n",
      "train_accuracy: 0.83\n",
      "test_accuracy: 0.6478405315614618\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "Fit time =  0.22610701799999333\n",
      "Prediction time =  0.03353718900001468\n",
      "precision: 0.6990794016110472\n",
      "recall: 0.6988505747126437\n",
      "train_accuracy: 0.8457142857142858\n",
      "test_accuracy: 0.6976744186046512\n",
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CalcMetrics(RandomForest(),RandomForestClassifier(n_estimators=100, max_depth=5),X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты работы алгоритмов для второго датасета подтверждают всё, что было написано в выводах выше. Точность хорошая, но временные затраты просто космические. На данном примере видно, что, хоть результат (победа одной из команд) и зависел от имеющихся в датасете данных куда сильнее, чем в первом случае, эти закономерности смогли использовать только два метода: дерево решений и случайный лес, в то время как метрики KNN и логистической регрессии оказались даже хуже, чем в случае с шахматами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы по всей работе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для меня удивительными в данной работе оказались две противоположные вещи: 1. Собственные модели практически не уступали в точности предсказаний моделям и ScikitLearn. 2. Хоть я и ожидал, что время работы будет хуже, но чтобы в 100, а в худшем случае и почти в 500 раз медленнее.\n",
    "Очевидно, что в моделях из пакета применялись различные оптимизации, но чтобы настолько сокращать время работы, это действительно удивительно. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
